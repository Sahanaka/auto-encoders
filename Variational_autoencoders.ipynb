{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Variational_autoencoders.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNCydWSejavbCausXKpt3fw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahanaka/auto-encoders/blob/main/Variational_autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypbXslEP5EVi"
      },
      "source": [
        "# **Variational AutoEncoders**\n",
        "**In this notebook we will build a Variational Autoencoder (VAE) trained on the MNIST dataset and see how it is able to generate new images.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgLRUbzs5XGx"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb8sFcuM2sAk"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CcO1SLe5rA7"
      },
      "source": [
        "# Define global constants to be used in this notebook\n",
        "BATCH_SIZE = 128\n",
        "LATENT_DIM = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FFZsCWt5DT3"
      },
      "source": [
        "## **Prepare the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J21JQ0LJ5chA"
      },
      "source": [
        "def map_image(image, label):\n",
        "  '''returns a normalized and reshaped tensor from a given image'''\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = image / 255.0\n",
        "  image = tf.reshape(image, shape=(28, 28, 1,))\n",
        "  \n",
        "  return image\n",
        "\n",
        "\n",
        "def get_dataset(map_fn, is_validation=False):\n",
        "  '''Loads and prepares the mnist dataset from TFDS.'''\n",
        "  if is_validation:\n",
        "    split_name = \"test\"\n",
        "  else:\n",
        "    split_name = \"train\"\n",
        "\n",
        "  dataset = tfds.load('mnist', as_supervised=True, split=split_name)\n",
        "  dataset = dataset.map(map_fn)\n",
        "  \n",
        "  if is_validation:\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "  else:\n",
        "    dataset = dataset.shuffle(1024).batch(BATCH_SIZE)\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZaftAf353iq"
      },
      "source": [
        "train_dataset = get_dataset(map_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HuPfRc05dNJ"
      },
      "source": [
        "## **Model**\n",
        "\n",
        "The main parts are shown in the figure below:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1YAZAeMGEJ1KgieYk1ju-S9DoshpMREeC\" width=\"60%\" height=\"60%\"/>\n",
        "\n",
        "VAE also has an encoder-decoder architecture with the main difference being the grey box in the middle which stands for the latent representation. In this layer, the model mixes a random sample and combines it with the outputs of the encoder. This mechanism makes it useful for generating new content. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ch2j-0m6cnR"
      },
      "source": [
        "### Sampling Class\n",
        "\n",
        "First, we will build the `Sampling` class. This will be a custom Keras layer that will provide the Gaussian noise input along with the mean (mu) and standard deviation (sigma) of the encoder's output. In practice, the output of this layer is given by the equation:\n",
        "\n",
        "$$z = \\mu + e^{0.5\\sigma} * \\epsilon  $$\n",
        "\n",
        "where $\\mu$ = mean, $\\sigma$ = standard deviation, and $\\epsilon$ = random sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-9NVbmE5jqp"
      },
      "source": [
        "class Sampling(tf.keras.layers.Layer):\n",
        "  def call(self, inputs):\n",
        "    mu, sigma = inputs\n",
        "\n",
        "    # Get the size and dimensions of the batch\n",
        "    batch = tf.shape(mu)[0]\n",
        "    dim = tf.shape(mu)[1]\n",
        "\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim)) # Random tensor\n",
        "    return mu + tf.exp(0.5 * sigma) * epsilon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr_2h6OD7BPT"
      },
      "source": [
        "### Encoder\n",
        "\n",
        "Next, we will build the encoder part of the network. We will follow the architecture shown in class which looks like this. Note that aside from mu and sigma, we will also output the shape of features before flattening it. This will be useful when reconstructing the image later in the decoder.\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1eoxFK_UVSHd3a_5EHcCU8F8QDZlPiXfW\" width=\"60%\" height=\"60%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJCtf6797AVL"
      },
      "source": [
        "def encoder_layers(inputs, latent_dim):\n",
        "  # Add the Conv2D layers followed by BatchNormalization\n",
        "  x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\", activation='relu', name=\"encode_conv1\")(inputs)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"encode_conv2\")(x)\n",
        "\n",
        "  # Assign to a different variable to extract the shape later\n",
        "  batch_2 = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  # Flatten the features and feed into the Dense network\n",
        "  x = tf.keras.layers.Flatten(name=\"encode_flatten\")(batch_2)\n",
        "\n",
        "  x = tf.keras.layers.Dense(20, activation='relu', name=\"encode_dense\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  # Add output Dense networks for mu and sigma, units equal to the declared latent_dim.\n",
        "  mu = tf.keras.layers.Dense(latent_dim, name='latent_mu')(x)\n",
        "  sigma = tf.keras.layers.Dense(latent_dim, name='latent_sigma')(x)\n",
        "\n",
        "  return mu, sigma, batch_2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSFYcjaZ9EsC"
      },
      "source": [
        "**Now lets create the Encoder with the Sampling layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-1FbFFj9Eb5"
      },
      "source": [
        "def encoder_model(latent_dim, input_shape):\n",
        "  inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "  # Get the output of the encoder_layers() function\n",
        "  mu, sigma, conv_shape = encoder_layers(inputs, latent_dim=LATENT_DIM)\n",
        "\n",
        "  # feed mu and sigma to the Sampling layer\n",
        "  z = Sampling()((mu, sigma))\n",
        "\n",
        "  # build the whole encoder model\n",
        "  model = tf.keras.Model(inputs, outputs=[mu, sigma, z])\n",
        "\n",
        "  return model, conv_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKlKD_UZ9iR6"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "The decoder part of the network expands the latent representations back to the original image dimensions. Later in the training loop, we can feed random inputs to this model and it will generate content that resemble the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNhKAQuF85Ad"
      },
      "source": [
        "def decoder_layers(inputs, conv_shape):\n",
        "  # Feed to a Dense network with units computed from the conv_shape dimensions\n",
        "  units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n",
        "  x = tf.keras.layers.Dense(units, activation = 'relu', name=\"decode_dense1\")(inputs)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  \n",
        "  # Reshape output using the conv_shape dimensions\n",
        "  x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]), name=\"decode_reshape\")(x)\n",
        "\n",
        "  # Upsample the features back to the original dimensions\n",
        "  x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_2\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_3\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same', activation='sigmoid', name=\"decode_final\")(x)\n",
        "  \n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJo_nNen99x5"
      },
      "source": [
        "def decoder_model(latent_dim, conv_shape):\n",
        "  # Set the inputs to the shape of the latent space\n",
        "  inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "\n",
        "  # Get the output of the decoder layers\n",
        "  outputs = decoder_layers(inputs, conv_shape)\n",
        "\n",
        "  # Declare the inputs and outputs of the model\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCux9403-Pdi"
      },
      "source": [
        "### Kullback–Leibler Divergence\n",
        "To improve the generative capability of the model, we have to take into account the random normal distribution introduced in the latent space. For that, the [Kullback–Leibler Divergence](https://arxiv.org/abs/2002.07514) is computed and added to the reconstruction loss. The formula is defined in the function below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zPciUau-KLi"
      },
      "source": [
        "def kl_reconstruction_loss(inputs, outputs, mu, sigma):\n",
        "  \"\"\" Computes the Kullback-Leibler Divergence (KLD)\n",
        "  Args:\n",
        "    inputs -- batch from the dataset\n",
        "    outputs -- output of the Sampling layer\n",
        "    mu -- mean\n",
        "    sigma -- standard deviation\n",
        "\n",
        "  Returns:\n",
        "    KLD loss\n",
        "  \"\"\"\n",
        "  kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
        "  kl_loss = tf.reduce_mean(kl_loss) * - 0.5\n",
        "\n",
        "  return kl_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uapIGKMU-a7K"
      },
      "source": [
        "### **VAE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q6opWBL-Ybk"
      },
      "source": [
        "def vae_model(encoder, decoder, input_shape):\n",
        "  inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "  mu, sigma, z = encoder(inputs)\n",
        "\n",
        "  # Get reconstructed output from the decoder\n",
        "  reconstructed = decoder(z)\n",
        "\n",
        "  # Define the model\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=reconstructed)\n",
        "\n",
        "  # Add the KL loss\n",
        "  loss = kl_reconstruction_loss(inputs, z, mu, sigma)\n",
        "  model.add_loss(loss)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcrE0oLA_Gjk"
      },
      "source": [
        "def get_models(input_shape, latent_dim):\n",
        "  \"\"\"Returns the encoder, decoder, and vae models\"\"\"\n",
        "  encoder, conv_shape = encoder_model(latent_dim=latent_dim, input_shape=input_shape)\n",
        "  decoder = decoder_model(latent_dim=latent_dim, conv_shape=conv_shape)\n",
        "  vae = vae_model(encoder, decoder, input_shape=input_shape)\n",
        "  return encoder, decoder, vae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyBi-DHG_NnJ"
      },
      "source": [
        "# Get the encoder, decoder and 'master' model (called vae)\n",
        "encoder, decoder, vae = get_models(input_shape=(28,28,1,), latent_dim=LATENT_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFnNNIBK__jb"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zv_7o1p_QYi"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_metric = tf.keras.metrics.Mean()\n",
        "bce_loss = tf.keras.losses.BinaryCrossentropy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZIaMiEhARW0"
      },
      "source": [
        "We will see the progress of the image generation at each epoch. For that, we can use the helper function below. This will generate 16 images in a 4x4 grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJHmwOYoANlf"
      },
      "source": [
        "def generate_and_save_images(model, epoch, step, test_input):\n",
        "  # generate images from the test input\n",
        "  predictions = model.predict(test_input)\n",
        "\n",
        "  # plot the results\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  # tight_layout minimizes the overlap between 2 sub-plots\n",
        "  fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n",
        "  plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUY828IvAicj"
      },
      "source": [
        "#### Custom Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p-S1JMHAbdC"
      },
      "source": [
        "# Training loop. \n",
        "\n",
        "# generate random vector as test input to the decoder\n",
        "random_vector_for_generation = tf.random.normal(shape=[16, LATENT_DIM])\n",
        "\n",
        "# number of epochs\n",
        "epochs = 100\n",
        "\n",
        "# initialize the helper function to display outputs from an untrained model\n",
        "generate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print('Start of epoch %d' % (epoch,))\n",
        "\n",
        "  # iterate over the batches of the dataset.\n",
        "  for step, x_batch_train in enumerate(train_dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      # feed a batch to the VAE model\n",
        "      reconstructed = vae(x_batch_train)\n",
        "\n",
        "      # compute reconstruction loss\n",
        "      flattened_inputs = tf.reshape(x_batch_train, shape=[-1])\n",
        "      flattened_outputs = tf.reshape(reconstructed, shape=[-1])\n",
        "      loss = bce_loss(flattened_inputs, flattened_outputs) * 784\n",
        "      \n",
        "      # add KLD regularization loss\n",
        "      loss += sum(vae.losses)  \n",
        "\n",
        "    # get the gradients and update the weights\n",
        "    grads = tape.gradient(loss, vae.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
        "\n",
        "    # compute the loss metric\n",
        "    loss_metric(loss)\n",
        "\n",
        "    # display outputs every 100 steps\n",
        "    if step % 100 == 0:\n",
        "      display.clear_output(wait=False)    \n",
        "      generate_and_save_images(decoder, epoch, step, random_vector_for_generation)\n",
        "      print('Epoch: %s step: %s mean loss = %s' % (epoch, step, loss_metric.result().numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQuZfCGQAtrr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}